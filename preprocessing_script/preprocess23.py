# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z6RbwWSmwLjWr0N-Z03E5dMuMMDTf5UC
"""

#!pip install stanza
data = "eLife"
split = "validation"
repo = f"/content/drive/MyDrive/biolaysumm/{data}_{split}.json"
from google.colab import drive
drive.mount('/content/drive')
import json
# Perform standard imports:
import spacy
nlp = spacy.load('en_core_web_sm')
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwds = set(stopwords.words('english')) # all stopwords in english language
print(stopwds)
from sentence_transformers import SentenceTransformer
import numpy as np
import torch
nltk.download('punkt')
#import stanza
#stanza.download('en', package='genia') #bionlp package
#nlp = stanza.Pipeline('en', processors='tokenize', package='genia')


model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")

#returns a tuple of sentences, and
def compute_similarity(keyword_embedding, all_sentences, sentence_embeddings):
  all_similarities = []
  for sentence_embedding in sentence_embeddings:
    #print(sentence_embedding.shape)
    #print(keyword_embedding.shape)

    cos_sim = np.dot(keyword_embedding, sentence_embedding)/(np.linalg.norm(keyword_embedding)*np.linalg.norm(sentence_embedding))

    all_similarities.append(cos_sim)

  pair_sentence_similarity = list(zip(all_sentences, all_similarities, sentence_embeddings))
  #print(pair_sentence_similarity)
  #sort based on similarity
  pair_sentence_similarity.sort(key=lambda x: x[1], reverse=True)
  return pair_sentence_similarity


def get_top_p(pair_sentence_similarity, p):
  n = len(pair_sentence_similarity)
  top_p_sentences = [tuple[0] for tuple in pair_sentence_similarity[:int(n*p)]]
  return top_p_sentences

def get_top_k(pair_sentence_similarity, p):
  n = len(pair_sentence_similarity)
  top_k_sentences = [tuple[0] for tuple in pair_sentence_similarity[:p]]
  return top_k_sentences


def compare_summary(top_p_sentences, summary):
  embedding_top_p = np.mean(model.encode(top_p_sentences), axis=0)
  embedding_summary = np.mean(model.encode(summary), axis=0)
  cos_sim = np.dot(embedding_top_p, embedding_summary)/(np.linalg.norm(embedding_top_p)*np.linalg.norm(embedding_summary))
  return cos_sim


def get_random_p_sentences(all_sentences, p):
  n = len(all_sentences)
  random_p_sentences = np.random.choice(all_sentences, int(n*p), replace=False)
  return random_p_sentences

import re

def clean_text(text):
    text = text.replace(' . ', '. ').replace(' , ', ', ')
    text = text.replace('\n\n', ' ')
    text = re.sub(r'\s\[.*?\]', "", text)
    text = re.sub(r'(\(\s([^()]*\s\,\s)*[^()]*\s\))', "", text)
    return text

def is_valid_sentence(text):
  doc = nlp(text)
  # Check if the sentence has at least one alphabet character
  has_alphabets = bool(re.search('[a-zA-Z]', text))
  return len(list(doc.sents)) > 0 and len(doc) > 5 and has_alphabets

with open(repo, 'r', encoding='utf-8') as file:
    data = json.load(file)

output=[]
total_samples = len(data)
total_best = 0
for index, item in enumerate(data):
    keyword_sentence = {}
    keywords = set(item.get('keywords'))
    #remove stopwords from keywords via set_difference
    try:

      keywords = keywords.difference(stopwds)
      '''
      keyword_embedding = model.encode(list(keywords)) # matrix of embeddings for each keyword
      keyword_embedding = keyword_embedding.mean(axis=0)
      keyword_embedding = torch.from_numpy(keyword_embedding)
      '''
      sentence=item['title']
      #print(sentence)
      title_tokens = set(sentence.split())
      #print(title_tokens)
      title_tokens = title_tokens.difference(stopwds).union(keywords)
      title_embedding = model.encode(list(title_tokens))
      title_embedding = title_embedding.mean(axis=0)
      title_embedding = torch.from_numpy(title_embedding)
      #print(title_embedding)

      clean_summary = clean_text(item['summary'])

      summary = nlp(clean_summary)
      summary_sentences = [sent.text.strip() for sent in summary.sents if len(sent.text.strip()) > 0]
      filtered_summary_sentences = [sentence for sentence in summary_sentences if is_valid_sentence(sentence)]
      print(f'summary_sentences:{summary_sentences}')
      print(f'filtered_summary_sentences:{filtered_summary_sentences}')



      clean_article = clean_text(item['article'])
      article = nlp(clean_article)
      print(article)
      article_sentences = [sent.text.strip() for sent in article.sents if len(sent.text.strip()) > 0]
      filtered_article_sentences = [sentence for sentence in article_sentences if is_valid_sentence(sentence)]

      sentence_embeddings = model.encode(filtered_article_sentences) #returns a tensor that contains all sentence embeddings
      pair_sentence_similarity = compute_similarity(title_embedding, filtered_article_sentences, sentence_embeddings)

      print(len(pair_sentence_similarity))
      #choose top-k sentences - 60%

      top_p_sentences = get_top_k(pair_sentence_similarity, 40)
      print(f'top_p_sentences: {top_p_sentences}')
      top_p_join = "  ".join(top_p_sentences)
      output.append({'preprocessed article':top_p_join, 'gold summary': item['summary']})
      print(f'top_p_join: {top_p_join}')

      top_p_cos_summary = compare_summary(top_p_sentences, summary_sentences)
      #print(top_p_cos_summary)

      ##### code to randomly select sentences

      random_p_sentences = get_random_p_sentences(article_sentences, 0.3)

      random_p_cos_summary = compare_summary(random_p_sentences, summary_sentences)
      #print(random_p_cos_summary)

      print(f'index: {index}')
      print(f'random_p_cos_summary: {random_p_cos_summary}')
      print(f'top_p_cos_summary: {top_p_cos_summary}')
      print(f'/n')

      if random_p_cos_summary < top_p_cos_summary:
        total_best+=1
    except:
      print('error')





print(f'Number of data samples that did better with top_p: {total_best}')
print(f'total number of data samples: {total_samples}')

print(len(output))
configuration='title_keywords_top40'
repo = f"/content/drive/MyDrive/biolaysumm/processed_data_{configuration}.json"

with open (repo, 'w') as of:
  json.dump(output, of)



"""Results for PLOS_validation without using titles, and skipping instances that do not have any keywords:
Number of data samples that did better with top_p: 848
total number of data samples: 1376
Around 61.6% - not good

Number of data samples that did better with
 top_p: 1120
total number of data samples: 1376

There is an issue with sentence-segmentation. Not getting clean sentences. - get LLMs to do it?

"""

