# -*- coding: utf-8 -*-
"""SVD_Approach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LvEU5acmsbQHbewIV2NFTAdG5aaB19cK
"""

import spacy
nlp = spacy.load('en_core_web_sm')
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import nltk
import re

from sentence_transformers import SentenceTransformer
model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")
import json

data = "eLife"
split = "validation"
repo = f"/content/drive/MyDrive/biolaysumm/{data}_{split}.json"
from google.colab import drive
drive.mount('/content/drive')

nltk.download('punkt')
nltk.download('punkt_tab')

def cosine_similarity(top_p_sentences, summary, model):
  # Check if top_p_sentences is empty or contains only one sentence
  if not top_p_sentences or len(top_p_sentences) == 1:
    # If so, duplicate the sentence to avoid issues with mean calculation
    top_p_sentences = top_p_sentences * 2 if top_p_sentences else ["", ""]
  embedding_top_p = np.mean(model.encode(top_p_sentences), axis=0).reshape(1, -1)  # Reshape to (1, 768)
  embedding_summary = np.mean(model.encode(summary), axis=0).reshape(1, -1)  # Reshape to (1, 768)
  cos_sim = np.dot(embedding_top_p, embedding_summary.T)/(np.linalg.norm(embedding_top_p)*np.linalg.norm(embedding_summary)) # Transpose embedding_summary and calculate dot product
  return cos_sim

def clean_text(text):
    text = text.replace(' . ', '. ').replace(' , ', ', ')
    text = text.replace('\n\n', ' ')
    text = re.sub(r'\s\[.*?\]', "", text)
    text = re.sub(r'(\(\s([^()]*\s\,\s)*[^()]*\s\))', "", text)
    return text

def is_valid_sentence(text):
  doc = nlp(text)
  # Check if the sentence has at least one alphabet character
  has_alphabets = bool(re.search('[a-zA-Z]', text))
  return len(list(doc.sents)) > 0 and len(doc) > 5 and has_alphabets


def reorganize_data_by_topic(data):
  all_data_reorganized_by_topic = []
  for data_sample in data:
    article = clean_text(data_sample['article'])
    gold_summary = data_sample['summary']
    summary = nlp(gold_summary)
    sample = {'gold_summary': [sentence.text for sentence in summary.sents if is_valid_sentence(sentence.text)]}
    # Split article into sentences
    doc = nlp(article)
    # sentence-term matrix
    sentences = [sentence.text for sentence in doc.sents if is_valid_sentence(sentence.text)]
    vectorizer = TfidfVectorizer(stop_words='english')
    sentence_term_matrix = vectorizer.fit_transform(sentences)
    # SVD
    n_topics = 20  #Hyperparameter
    svd = TruncatedSVD(n_components=n_topics)
    sentence_topic_matrix = svd.fit_transform(sentence_term_matrix)
    # Get topics (for interpretation)
    topics = svd.components_
    terms = vectorizer.get_feature_names_out()
    # Print top terms for each topic (for your reference)
    for i, topic in enumerate(topics):
        top_terms_idx = topic.argsort()[-10:][::-1]  # Top 10 terms with highest weights
        top_terms = [terms[idx] for idx in top_terms_idx]
        print(f"Topic {i}: {', '.join(top_terms)}")

    # Assign each sentence to its dominant topic
    dominant_topics = np.argmax(sentence_topic_matrix, axis=1)
    # this dictionary represents an article whose sentences have been reorganized b
    sentence_groups = {}
    for topic_idx in range(n_topics):
        sentence_groups[topic_idx] = []

    for sent_idx, topic_idx in enumerate(dominant_topics):
        sentence_groups[topic_idx].append(sentences[sent_idx])

    # Print sentences grouped by topic
    for topic_idx, sentences_in_topic in sentence_groups.items():
        print(f"\nTopic {topic_idx} sentences:")
        for sent in sentences_in_topic:
            print(f"- {sent}")
    sample['article']=sentence_groups #sample is a dictionary that represents an article. It contains the gold summary, and another dictionary of topically grouped sentences
    all_data_reorganized_by_topic.append(sample)
  return all_data_reorganized_by_topic

def rerank_data(all_data_reorganized_by_topic):

  '''
  for each data sample, compare the contents of the topic against the gold summary, and rerank the topics
  '''
  reranked_data = []
  for data_sample in all_data_reorganized_by_topic:
    gold_summary = data_sample['gold_summary']
    sentence_groups = data_sample['article'] #{gold_summary: gold_summary, article: {topic1:[sentences], topic2:[sentences]....}}
    for topic in sentence_groups:
      sentences_in_topic = sentence_groups[topic] #extracts list of sentences

      #compute cosine similarity score between topically-grouped sentences and the gold summary, and store it in a
      cos_sim = cosine_similarity(sentences_in_topic, gold_summary, model)
      #append it to the end of the list
      sentence_groups[topic].append(cos_sim)

    #sort sentence groups dictionary by largest cosine similarity - most similar topic-sentences
    #sorted_sentence_groups = dict(sorted(sentence_groups.items(), key=lambda x: x[1][-1], reverse=True))
    # Create a list of (topic, cos_sim) tuples for sorting
    topic_similarities = [(topic, sentences[-1]) for topic, sentences in sentence_groups.items()]

    # Sort the tuples by cos_sim in descending order
    sorted_topic_similarities = sorted(topic_similarities, key=lambda item: item[1], reverse=True)
    # Rebuild sentence_groups based on sorted topics, preserving the sentences
    sorted_sentence_groups = {}
    for topic, _ in sorted_topic_similarities:  # _ ignores the cos_sim value here
        sentences_with_cos_sim = sentence_groups[topic]  # Get the original list with cos_sim
        sorted_sentence_groups[topic] = sentences_with_cos_sim  # Add to sorted dictionary
    reranked_data.append({'gold_summary': gold_summary, 'article': sorted_sentence_groups})
    print(f'gold summary: {gold_summary}')
    print(f'article: {sorted_sentence_groups}')
  return reranked_data



with open(repo, 'r') as file:
  data = json.load(file)
configuration = 'svd_top40'
output_dir = f"/content/drive/MyDrive/biolaysumm/processed_data_{configuration}.txt"


all_data_reorganized_by_topic = reorganize_data_by_topic(data)
reranked_data = rerank_data(all_data_reorganized_by_topic)

def output_data_from_topk_topics(reranked_data, k, top_k_sents):
  count = 0
  output_to_dump_out = []

  for data in reranked_data:
    paragraph = ''
    sentences = data['article']

    top_k_items = list(data['article'].items())[:k]
    for topic in top_k_items: #tuple
      if count < top_k_sents:
        print(len(topic[1]))
        paragraph += " ".join(topic[1][:-1])
        top_k_sents +=1
      elif count == top_k_sents:
        break

    output_to_dump_out.append({'preprocessed article': paragraph, 'gold_summary': data['gold_summary']})
    print(count)
  return output_to_dump_out

k = 3
top_k_sents = 40
output_dir = f"/content/drive/MyDrive/biolaysumm/processed_data_{configuration}_.json"
output_data = output_data_from_topk_topics(reranked_data, k, top_k_sents)
with open(output_dir, 'w') as file:
  json.dump(output_data, file)

print(len(all_data_reorganized_by_topic))
print(all_data_reorganized_by_topic[0])

"""Looking at the sentences, there are many strange sentences.

Is there something wrong with the sent_tokenizer???

"""

print(len(all_data_reorganized_by_topic))