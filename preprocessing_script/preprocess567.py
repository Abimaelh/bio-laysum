# -*- coding: utf-8 -*-
"""preprocess567.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vQy-J_24fhlchsWnr4Wmn9HXbotI8LGk
"""

from google.colab import drive
import json
import random

# Step 1: Mount Google Drive
drive.mount('/content/drive')

# Step 2: Load entire file as text and parse manually
input_path = "/content/drive/MyDrive/biolaysumm/PLOS_train.json"
records = []
buffer = ""

with open(input_path, "r", encoding="utf-8") as f:
    for line in f:
        buffer += line.strip()
        if buffer.endswith("}"):  # crude check for end of object
            try:
                obj = json.loads(buffer)
                records.append(obj)
            except json.JSONDecodeError:
                pass
            buffer = ""

print(f"✅ Parsed {len(records)} total JSON objects.")

# Step 3: Sample 200 full articles (no filtering this time)
if len(records) >= 200:
    sampled = random.sample(records, 200)
else:
    raise ValueError(f"Only {len(records)} records available.")

# Step 4: Save output in proper format
output_path = "/content/drive/MyDrive/biolaysumm/PLOS_train_200sample.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(sampled, f, indent=2, ensure_ascii=False)

print(f"✅ Saved 200 random samples to: {output_path}")

#preprocess5.py

# Step 1: Mount Google Drive
from google.colab import drive
import os
import pandas as pd
import re
import spacy
from sentence_transformers import SentenceTransformer, util
import torch
import json

# Mount drive
drive.mount('/content/drive')

# Step 2: Prepare paths
json_path = '/content/drive/MyDrive/biolaysumm/eLife_train.json'

# Step 3: Load Dataset
if not os.path.exists(json_path):
    raise FileNotFoundError(f"JSON file not found at {json_path}")

df = pd.read_json(json_path)
print("✅ Dataset loaded!")

# Step 4: Flatten if needed
if isinstance(df.iloc[0], dict):
    df = pd.DataFrame(df.tolist())

# Step 5: Helper functions
def clean_text(text):
    if isinstance(text, str):
        return re.sub(r"\s+", " ", text).strip()
    return ""

def remove_parentheses_and_correct_spacing(text):
    if isinstance(text, str):
        # Remove nested parentheses content completely
        while re.search(r"\([^()]*\)", text):
            text = re.sub(r"\([^()]*\)", "", text)
        text = re.sub(r"\s+", " ", text)  # fix spacing
        return text.strip()
    return ""

if "article" not in df.columns:
    raise KeyError("'article' column not found in dataset.")

# Apply helper function first
df["article"] = df["article"].apply(remove_parentheses_and_correct_spacing)
# Then apply basic cleaning
df["article"] = df["article"].apply(clean_text)

summary_field = "summary"
if summary_field not in df.columns:
    raise KeyError(f"'{summary_field}' column not found in dataset.")
df[summary_field] = df[summary_field].apply(clean_text)

# Step 6: Sample 200 instances
sampled_df = df.sample(n=200, random_state=42).reset_index(drop=True)

# Step 7: Download and Load NLP Models
!python -m spacy download en_core_web_sm
nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")

# Step 8: Select top 40 sentences by cosine similarity to article average
selected_passages = []

for article in sampled_df["article"]:
    doc = nlp(article)
    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 0]

    if len(sentences) == 0:
        selected_passages.append("")
        continue

    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
    article_embedding = sentence_embeddings.mean(dim=0, keepdim=True)
    similarities = util.pytorch_cos_sim(article_embedding, sentence_embeddings)[0]

    top_k = min(40, len(sentences))
    top_indices = similarities.argsort(descending=True)[:top_k]
    selected = " ".join([sentences[i] for i in sorted(top_indices.tolist())])
    selected_passages.append(selected)

# Step 9: Create Final Output
output_df = pd.DataFrame({
    "preprocessed_article": selected_passages,
    "gold_summary": sampled_df[summary_field]
})

# Step 10: Save Output
save_path = "/content/drive/MyDrive/BioLaySum/preprocessed_article/5_eLife_train_200sample.json"
os.makedirs(os.path.dirname(save_path), exist_ok=True)
output_df.to_json(save_path, orient="records", indent=2, force_ascii=False)

print(f"Preprocessed sampled dataset saved to: {save_path}")

# preprocess 5 (new)

from google.colab import drive
drive.mount('/content/drive')


import json

with open("/content/drive/MyDrive/biolaysumm/PLOS_train_200sample.json", "r", encoding="utf-8") as f:
    raw = f.read()

print("First 300 characters:\n", raw[:300])

import json
import re
import spacy
import torch
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util

# Load NLP tools
nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")



# Helper: remove nested parentheses and fix spacing
def clean_text(text):
    if not isinstance(text, str):
        return ""
    # Remove nested parentheses
    while re.search(r"\([^()]*\)", text):
        text = re.sub(r"\([^()]*\)", "", text)
    # Fix extra whitespace
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# Load 200-sample JSON file
input_path = "/content/drive/MyDrive/biolaysumm/PLOS_train_200sample.json"
data = []
with open(input_path, "r", encoding="utf-8") as f:
    raw = json.load(f)  # This loads the outer list

    for entry in raw:
        if isinstance(entry, str):
            try:
                entry = json.loads(entry)  # Decode inner string
            except json.JSONDecodeError:
                continue
        if isinstance(entry, dict):
            data.append(entry)

print(f"✅ Loaded {len(data)} valid records.")

# Process each article
results = []
for entry in tqdm(data, desc="Processing"):
    article = clean_text(entry.get("article", ""))
    summary = clean_text(entry.get("summary", ""))

    # Sentence splitting
    doc = nlp(article)
    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]

    if not sentences:
        results.append({"article": "", "summary": summary})
        continue

    # Embedding
    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
    avg_embedding = sentence_embeddings.mean(dim=0, keepdim=True)
    similarities = util.pytorch_cos_sim(avg_embedding, sentence_embeddings)[0]

    # Select top 40
    top_k = min(40, len(sentences))
    top_indices = similarities.argsort(descending=True)[:top_k]
    top_sentences = " ".join([sentences[i] for i in sorted(top_indices.tolist())])

    # Store result
    results.append({
        "article": top_sentences,
        "summary": summary
    })

# Save output
output_path = "/content/drive/MyDrive/BioLaySum/preprocessed_article/5_PLOS_train_200sample.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"✅ Saved: {output_path}")

# preprocess 6(new)

# Step 1: Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Import Libraries
import os
import re
import spacy
import torch
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util
import json

# Step 3: Load NLP Tools
nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")

# Step 4: Clean Text (remove parentheses + spacing)
def clean_text(text):
    if isinstance(text, str):
        while re.search(r"\([^()]*\)", text):
            text = re.sub(r"\([^()]*\)", "", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    return ""

# Step 5: Robustly Load JSON File
file_path = "/content/drive/MyDrive/biolaysumm/PLOS_train.json"
records = []
with open(file_path, "r", encoding="utf-8") as f:
    buffer = ""
    for line in f:
        buffer += line.strip()
        if buffer.endswith("}"):
            try:
                records.append(json.loads(buffer))
            except json.JSONDecodeError:
                pass
            buffer = ""

df = pd.DataFrame(records)
print(f"✅ Loaded {len(df)} records.")

# Ensure required fields
required_cols = ["article", "summary", "section_headings", "title", "keywords"]
missing = [col for col in required_cols if col not in df.columns]
if missing:
    raise KeyError(f"❌ Missing required columns: {missing}")

# Step 6: Clean text fields
df["article"] = df["article"].apply(clean_text)
df["summary"] = df["summary"].apply(clean_text)

# Step 7: Preprocessing function (title, keywords, sections)
def preprocess_article(article_text, section_headings, title, keywords,
                       target_sections=["abstract", "introduction", "results", "discussion"],
                       top_k=40):
    header = f"Title: {title.strip()}\nKeywords: {', '.join(keywords).strip() if isinstance(keywords, list) else keywords.strip()}"
    full_text = header + "\n\n" + article_text
    paragraphs = [p.strip() for p in full_text.split('\n') if p.strip()]
    n_paragraphs = len(paragraphs)
    n_sections = len(section_headings)
    chunk_size = max(1, n_paragraphs // max(n_sections, 1))

    section_chunks = []
    for i, heading in enumerate(section_headings):
        start = i * chunk_size
        end = (i + 1) * chunk_size if i < n_sections - 1 else n_paragraphs
        chunk_text = "\n".join(paragraphs[start:end])
        section_chunks.append((heading.strip().lower(), chunk_text))

    selected_body = "\n\n".join(text for name, text in section_chunks if name in target_sections)

    doc = nlp(selected_body)
    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
    if not sentences:
        return selected_body

    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
    avg_embedding = sentence_embeddings.mean(dim=0, keepdim=True)
    similarities = util.pytorch_cos_sim(avg_embedding, sentence_embeddings)[0]
    top_indices = similarities.argsort(descending=True)[:min(top_k, len(sentences))]
    return " ".join([sentences[i] for i in sorted(top_indices.tolist())])

# Step 8: Sample 200 entries
df = df.sample(n=min(200, len(df)), random_state=42).reset_index(drop=True)

# Step 9: Apply preprocessing
processed_articles = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    processed = preprocess_article(
        article_text=row["article"],
        section_headings=row["section_headings"],
        title=row["title"],
        keywords=row["keywords"]
    )
    processed_articles.append(processed)

# Step 10: Save result
output_df = pd.DataFrame({
    "preprocessed_article": processed_articles,
    "gold_summary": df["summary"]
})

save_path = "/content/drive/MyDrive/BioLaySum/preprocessed_article/6_PLOS_train_200sample.json"
os.makedirs(os.path.dirname(save_path), exist_ok=True)
output_df.to_json(save_path, orient="records", indent=2, force_ascii=False)
print(f"✅ Saved to: {save_path}")

#preprocess 7 (new)

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Import libraries
import os
import re
import spacy
import torch
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util
import json

# Load NLP tools
nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")

# Clean text by removing parentheses and extra spaces
def clean_text(text):
    if isinstance(text, str):
        while re.search(r"\([^()]*\)", text):
            text = re.sub(r"\([^()]*\)", "", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    return ""

# Extract specific sections from article
def extract_target_sections(article_text, section_headings,
                            target_sections=["abstract", "introduction", "results", "discussion"]):
    paragraphs = [p.strip() for p in article_text.split('\n') if p.strip()]
    n_paragraphs = len(paragraphs)
    n_sections = len(section_headings)
    chunk_size = max(1, n_paragraphs // max(n_sections, 1))

    section_chunks = []
    for i, heading in enumerate(section_headings):
        start = i * chunk_size
        end = (i + 1) * chunk_size if i < n_sections - 1 else n_paragraphs
        chunk_text = "\n".join(paragraphs[start:end])
        section_chunks.append((heading.strip().lower(), chunk_text))

    return "\n\n".join(text for name, text in section_chunks if name in target_sections)

# Select top-k representative sentences
def get_top_k_sentences(text, k=40):
    doc = nlp(text)
    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
    if not sentences:
        return ""
    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
    avg_embedding = sentence_embeddings.mean(dim=0, keepdim=True)
    similarities = util.pytorch_cos_sim(avg_embedding, sentence_embeddings)[0]
    top_indices = similarities.argsort(descending=True)[:min(k, len(sentences))]
    return " ".join([sentences[i] for i in sorted(top_indices.tolist())])

# 🧩 FIXED: Load JSON file with multiple concatenated objects
file_path = "/content/drive/MyDrive/biolaysumm/PLOS_train.json"
records = []
with open(file_path, "r", encoding="utf-8") as f:
    buffer = ""
    for line in f:
        buffer += line.strip()
        if buffer.endswith("}"):
            try:
                records.append(json.loads(buffer))
            except json.JSONDecodeError:
                pass
            buffer = ""

df = pd.DataFrame(records)
print(f"✅ Loaded {len(df)} records.")

# Check required columns
for col in ["article", "summary", "section_headings", "title", "keywords"]:
    if col not in df.columns:
        raise KeyError(f"Missing required column: {col}")

# Clean text
df["article"] = df["article"].apply(clean_text)
df["summary"] = df["summary"].apply(clean_text)

# Sample
df = df.sample(n=min(200, len(df)), random_state=42).reset_index(drop=True)

# Process all
processed_articles = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    section_body = extract_target_sections(
        article_text=row["article"],
        section_headings=row["section_headings"]
    )
    top_sentences = get_top_k_sentences(section_body)

    header = f"Title: {row['title'].strip()}\nKeywords: {', '.join(row['keywords']).strip() if isinstance(row['keywords'], list) else row['keywords'].strip()}"
    final_text = header + "\n\n" + top_sentences
    processed_articles.append(final_text)

# Save output
output_df = pd.DataFrame({
    "preprocessed_article": processed_articles,
    "gold_summary": df["summary"]
})

save_path = "/content/drive/MyDrive/BioLaySum/preprocessed_article/7_PLOS_train_200sample.json"
os.makedirs(os.path.dirname(save_path), exist_ok=True)
output_df.to_json(save_path, orient="records", indent=2, force_ascii=False)
print(f"✅ Saved to {save_path}")

#preprocess6.py

# Step 1: Mount Google Drive
from google.colab import drive
import os
import re
import spacy
import torch
import pandas as pd
from sentence_transformers import SentenceTransformer, util
from huggingface_hub import login

# Step 1: Authenticate with Hugging Face
login(token= "Your token here")

# Step 3: Load NLP tools
nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")

# Step 4: Helper to remove parentheses and fix spacing
def remove_parentheses_and_correct_spacing(text):
    if isinstance(text, str):
        while re.search(r"\([^()]*\)", text):
            text = re.sub(r"\([^()]*\)", "", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    return ""

# Step 5: Main preprocessing function
def preprocess_article(article_text, section_headings, title, keywords,
                       target_sections=["abstract", "introduction", "results", "discussion"],
                       top_k=40):
    # Prepend Title and Keywords
    header = f"Title: {title.strip()}\nKeywords: {', '.join(keywords).strip() if isinstance(keywords, list) else keywords.strip()}"
    full_text = header + "\n\n" + article_text

    # Chunk text into sections
    paragraphs = [p.strip() for p in full_text.split('\n') if p.strip()]
    n_paragraphs = len(paragraphs)
    n_sections = len(section_headings)

    if n_sections == 0 or n_paragraphs == 0:
        selected_body = full_text
    else:
        chunk_size = max(1, n_paragraphs // n_sections)
        section_chunks = []
        for i, heading in enumerate(section_headings):
            start = i * chunk_size
            end = (i + 1) * chunk_size if i < n_sections - 1 else n_paragraphs
            chunk_text = "\n".join(paragraphs[start:end])
            section_chunks.append((heading.strip().lower(), chunk_text))
        selected_body = "\n\n".join(text for name, text in section_chunks if name in target_sections)

    # Top-K representative sentences
    doc = nlp(selected_body)
    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
    if not sentences:
        selected_summary = selected_body
    else:
        sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
        avg_embedding = sentence_embeddings.mean(dim=0, keepdim=True)
        similarities = util.pytorch_cos_sim(avg_embedding, sentence_embeddings)[0]
        top_k = min(top_k, len(sentences))
        top_indices = similarities.argsort(descending=True)[:top_k]
        selected_summary = " ".join([sentences[i] for i in sorted(top_indices.tolist())])

    return selected_summary

# Step 6: Load dataset
df = pd.read_json("/content/drive/MyDrive/biolaysumm/eLife_train.json")

# Step 7: Flatten if needed
if isinstance(df.iloc[0], dict):
    df = pd.DataFrame(df.tolist())

# Step 8: Clean parentheses and spacing
df["article"] = df["article"].apply(remove_parentheses_and_correct_spacing)

# Step 9: Sample 200 articles
df = df.sample(n=200, random_state=42).reset_index(drop=True)

# Step 10: Check required fields
required = ["article", "summary", "section_headings", "title", "keywords"]
assert all(col in df.columns for col in required), f"Missing required columns: {required}"

# Step 11: Process articles
processed_articles = []
for _, row in df.iterrows():
    final_summary = preprocess_article(
        article_text=row["article"],
        section_headings=row["section_headings"],
        title=row["title"],
        keywords=row["keywords"]
    )
    processed_articles.append(final_summary)

# Step 12: Save output
output_df = pd.DataFrame({
    "preprocessed_article": processed_articles,
    "gold_summary": df["summary"]
})

output_path = "/content/drive/MyDrive/BioLaySum/preprocessed_article/eLife_train_sampled_titlekeywords_top40in4sections.json"
os.makedirs(os.path.dirname(output_path), exist_ok=True)
output_df.to_json(output_path, orient="records", indent=2, force_ascii=False)

print(f"Preprocessed and saved to: {output_path}")



#preprocess7.py

# Step 1: Mount Google Drive
from google.colab import drive
import os
import pandas as pd
import re
import spacy
import torch
from sentence_transformers import SentenceTransformer, util
from tqdm import tqdm

# Mount drive
drive.mount('/content/drive')

# Step 2: Prepare paths
json_path = '/content/drive/MyDrive/biolaysumm/eLife_train.json'

# Step 3: Load Dataset
if not os.path.exists(json_path):
    raise FileNotFoundError(f"JSON file not found at {json_path}")

df = pd.read_json(json_path)
print("Dataset loaded!")

# Step 4: Flatten if needed
if isinstance(df.iloc[0], dict):
    df = pd.DataFrame(df.tolist())

# Step 5: Helper functions
def clean_text(text):
    if isinstance(text, str):
        return re.sub(r"\s+", " ", text).strip()
    return ""

def remove_parentheses_and_correct_spacing(text):
    if isinstance(text, str):
        while re.search(r"\([^()]*\)", text):
            text = re.sub(r"\([^()]*\)", "", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    return ""

if "article" not in df.columns:
    raise KeyError("'article' column not found in dataset.")

# Apply helper function first
df["article"] = df["article"].apply(remove_parentheses_and_correct_spacing)
df["article"] = df["article"].apply(clean_text)

summary_field = "summary"
if summary_field not in df.columns:
    raise KeyError(f"'{summary_field}' column not found in dataset.")
df[summary_field] = df[summary_field].apply(clean_text)

# Step 6: Sample 200 instances
sampled_df = df.sample(n=200, random_state=42).reset_index(drop=True)

# Step 7: Download and Load NLP Models
!python -m spacy download en_core_web_sm
nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")

# Step 8: Section-aware extractor WITHOUT prepending title/keywords yet
def extract_target_sections(article_text, section_headings, target_sections=["abstract", "introduction", "results", "discussion"]):
    paragraphs = [p.strip() for p in article_text.split('\n') if p.strip()]

    n_paragraphs = len(paragraphs)
    n_sections = len(section_headings)

    if n_sections == 0 or n_paragraphs == 0:
        return "\n".join(paragraphs)

    chunk_size = max(1, n_paragraphs // n_sections)
    section_chunks = []

    for i, heading in enumerate(section_headings):
        start = i * chunk_size
        end = (i + 1) * chunk_size if i < n_sections - 1 else n_paragraphs
        chunk_text = "\n".join(paragraphs[start:end])
        section_chunks.append((heading.strip().lower(), chunk_text))

    filtered_chunks = [text for name, text in section_chunks if name in target_sections]
    return "\n\n".join(filtered_chunks)

# Step 9: Top-K sentence selector
def get_top_k_sentences(text, k=40):
    doc = nlp(text)
    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
    if not sentences:
        return ""

    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
    avg_embedding = sentence_embeddings.mean(dim=0, keepdim=True)
    similarities = util.pytorch_cos_sim(avg_embedding, sentence_embeddings)[0]

    top_k = min(k, len(sentences))
    top_indices = similarities.argsort(descending=True)[:top_k]
    return " ".join([sentences[i] for i in sorted(top_indices.tolist())])

# Step 10: Preprocess each article
processed_articles = []
for _, row in tqdm(sampled_df.iterrows(), total=len(sampled_df)):
    structured_text = extract_target_sections(
        article_text=row["article"],
        section_headings=row["section_headings"]
    )
    top_sentences = get_top_k_sentences(structured_text)

    # After picking top sentences, now prepend Title and Keywords
    header = f"Title: {row['title'].strip()}\nKeywords: {', '.join(row['keywords']).strip() if isinstance(row['keywords'], list) else row['keywords'].strip()}"
    final_text = header + "\n\n" + top_sentences

    processed_articles.append(final_text)

# Step 11: Create final output
output_df = pd.DataFrame({
    "preprocessed_article": processed_articles,
    "gold_summary": sampled_df["summary"]
})

# Step 12: Save Output
save_path = "/content/drive/MyDrive/BioLaySum/preprocessed_article/eLife_train_sampled_top40in4sections_titlekeywords.json"
os.makedirs(os.path.dirname(save_path), exist_ok=True)
output_df.to_json(save_path, orient="records", indent=2, force_ascii=False)

print(f"Preprocessed sampled dataset saved to: {save_path}")